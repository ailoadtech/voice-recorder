# Voice Intelligence Desktop App

A desktop application that captures voice input, transcribes it, and enriches it through AI-powered processing.

ATTENTION:
This application is fully KI generated by Kiro.dev and is a test project only to test code quality and output.
I dont use it and havn't tested in much detail. There are bugs and i will not fix them. Feel free to use to code.

## Features

- üé§ Voice recording with hotkey activation
- üìù AI-powered transcription using Whisper
- ‚ú® Text enrichment using GPT models
- üíæ Local storage and history
- üñ•Ô∏è Cross-platform desktop application

## Quick Start

### Prerequisites

- **Node.js 18+** and npm
- **OpenAI API key** - Get one at [platform.openai.com](https://platform.openai.com/api-keys)
- **WSL 2** (for Windows development) - See [WSL Development Guide](docs/WSL_DEVELOPMENT.md)

### Automated Setup (Recommended for WSL)

If you're developing in WSL, use the automated setup script:

# Run setup script
chmod +x scripts/wsl-setup.sh
./scripts/wsl-setup.sh
```

The script will:
- Verify Node.js and npm installation
- Install Git and build essentials if needed
- Configure file watchers for better performance
- Install npm dependencies
- Create `.env.local` from template
- Validate environment configuration

After the script completes, edit `.env.local` to add your OpenAI API key, then run `npm run dev`.

### Manual Setup

1. **Clone and install dependencies:**
   ```bash
   npm install
   ```

2. **Configure environment variables:**
   ```bash
   cp .env.example .env.local
   ```
   
   Edit `.env.local` and add your OpenAI API key:
   ```bash
   OPENAI_API_KEY=sk-your-api-key-here
   ```
   
   See [API Key Setup Guide](docs/API_KEY_SETUP.md) for detailed instructions.

3. **Validate your configuration:**
   ```bash
   npm run validate:api-keys
   ```

4. **Start development server:**
   ```bash
   npm run dev
   ```

5. **Access the application:**
   - Open your browser to `http://localhost:3000`
   - In WSL, the port is automatically forwarded to Windows

## Environment Configuration

The application uses environment variables for configuration. See [API Key Setup Guide](docs/API_KEY_SETUP.md) for detailed API key configuration and [Environment Setup Guide](docs/ENVIRONMENT_SETUP.md) for all other configuration options.

### Required Variables

- `OPENAI_API_KEY` - Your OpenAI API key for transcription and enrichment
  - Get your key at [platform.openai.com/api-keys](https://platform.openai.com/api-keys)
  - See [API Key Setup Guide](docs/API_KEY_SETUP.md) for step-by-step instructions

### Optional Variables

- `WHISPER_MODEL` - Whisper model to use (default: `whisper-1`)
- `GPT_MODEL` - GPT model to use (default: `gpt-4`)
- `HOTKEY_COMBINATION` - Global hotkey (default: `CommandOrControl+Shift+R`)
- And many more - see `.env.example` for all options

## LLM Provider Configuration

The application supports multiple LLM providers for text enrichment. You can choose between OpenAI's GPT API or a remote/local Ollama server.

### Switching Between Providers

Set the `LLM_PROVIDER` environment variable in your `.env.local` file:

```bash
# Use OpenAI (default)
LLM_PROVIDER=openai

# Use Ollama
LLM_PROVIDER=ollama
```

If `LLM_PROVIDER` is not set, the application defaults to OpenAI.

### OpenAI Provider (Default)

The OpenAI provider uses OpenAI's GPT API for text enrichment.

**Required Configuration:**
```bash
OPENAI_API_KEY=sk-your-api-key-here
```

**Optional Configuration:**
```bash
# Model selection (default: gpt-4)
NEXT_PUBLIC_GPT_MODEL=gpt-4

# Temperature for response randomness (0.0-2.0, default: 0.7)
GPT_TEMPERATURE=0.7

# Maximum tokens in response (default: 1000)
GPT_MAX_TOKENS=1000
```

**Supported Models:**
- `gpt-4` - Most capable model, best for complex tasks
- `gpt-4-turbo` - Faster and more cost-effective than GPT-4
- `gpt-3.5-turbo` - Fast and efficient for simpler tasks

### Ollama Provider

The Ollama provider allows you to use local or remote Ollama servers for text enrichment, providing more control over your data and infrastructure.

**Benefits:**
- üîí Privacy - Keep your data local
- üí∞ Cost-effective - No API usage fees
- üöÄ Customizable - Use any Ollama-compatible model
- üåê Flexible - Run locally or on a remote server

#### Setting Up Ollama

**1. Install Ollama Server**

On your local machine or remote server:

```bash
# Linux
curl -fsSL https://ollama.com/install.sh | sh

# macOS
brew install ollama

# Windows
# Download installer from https://ollama.com/download
```

**2. Start Ollama Service**

```bash
# Start Ollama server (runs on port 11434 by default)
ollama serve
```

**3. Pull a Model**

Download a model to use for text enrichment:

```bash
# Pull Llama 2 (7B - recommended for most users)
ollama pull llama2

# Or pull other models:
ollama pull llama3        # Meta's Llama 3 (8B)
ollama pull mistral       # Mistral 7B
ollama pull mixtral       # Mixtral 8x7B (larger, more capable)
ollama pull phi           # Microsoft Phi (2.7B - lightweight)
```

**4. Configure the Application**

Add these variables to your `.env.local` file:

```bash
# Select Ollama as the provider
LLM_PROVIDER=ollama

# Ollama server URL (default: http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# Model to use (default: llama2)
OLLAMA_MODEL=llama2

# Request timeout in milliseconds (default: 30000)
OLLAMA_TIMEOUT=30000
```

**5. Verify Connection**

Start the application and check the logs for successful Ollama connection:

```bash
npm run dev
```

You should see a message indicating the Ollama provider is initialized and healthy.

#### Remote Ollama Server

To use a remote Ollama server:

```bash
# In .env.local
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://192.168.1.100:11434  # Your server's IP
OLLAMA_MODEL=llama2
```

**Security Note:** Ensure your Ollama server is properly secured if exposing it over a network. Consider using a VPN or SSH tunnel for remote access.

#### Available Ollama Models

Popular models for text enrichment:

| Model | Size | Best For | Speed |
|-------|------|----------|-------|
| `llama2` | 7B | General purpose, balanced | Fast |
| `llama3` | 8B | Improved reasoning, latest | Fast |
| `mistral` | 7B | Efficient, high quality | Fast |
| `mixtral` | 8x7B | Complex tasks, best quality | Moderate |
| `phi` | 2.7B | Lightweight, quick responses | Very Fast |
| `codellama` | 7B | Code-related tasks | Fast |

**Model Selection Tips:**
- Start with `llama2` for a good balance of quality and speed
- Use `phi` for faster responses on lower-end hardware
- Use `mixtral` for the highest quality output (requires more resources)
- Check [ollama.com/library](https://ollama.com/library) for the full model catalog

#### Troubleshooting Ollama

**Connection refused:**
```bash
# Ensure Ollama is running
ollama serve

# Check if Ollama is listening
curl http://localhost:11434/api/tags
```

**Model not found (404 error):**
```bash
# List available models
ollama list

# Pull the model if not present
ollama pull llama2
```

**Slow responses:**
- Use a smaller model (e.g., `phi` instead of `mixtral`)
- Increase `OLLAMA_TIMEOUT` in `.env.local`
- Ensure sufficient RAM (8GB+ recommended for 7B models)

**Remote server not accessible:**
- Verify the server IP and port are correct
- Check firewall rules allow connections on port 11434
- Test connectivity: `curl http://<server-ip>:11434/api/tags`

## Development

### Available Scripts

- `npm run dev` - Start Next.js development server
- `npm run build` - Build for production
- `npm run start` - Start production server
- `npm run lint` - Run ESLint
- `npm run format` - Format code with Prettier
- `npm run validate-env` - Validate environment configuration
- `npm run validate:api-keys` - Validate API key configuration and test connection
- `npm run tauri:dev` - Run desktop app in development
- `npm run tauri:build` - Build desktop app for production
- `npm run sign:setup` - Setup code signing (Windows)
- `npm run sign:list` - List installed code signing certificates

### Dependency Management

Keep dependencies up-to-date and secure:

```bash
# Check for outdated packages
npm run deps:check

# Run security audit
npm run deps:audit

# Update patch versions (safe)
npm run deps:update

# Fix security vulnerabilities
npm run deps:fix

# Verify setup
npm run deps:verify
```

See [Dependency Management Guide](docs/DEPENDENCY_MANAGEMENT.md) for detailed information.

### Project Structure

```
/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ app/          # Next.js app directory (pages and routing)
‚îÇ   ‚îú‚îÄ‚îÄ components/   # React components
‚îÇ   ‚îú‚îÄ‚îÄ lib/          # Utility functions and helpers
‚îÇ   ‚îú‚îÄ‚îÄ services/     # External service integrations
‚îÇ   ‚îî‚îÄ‚îÄ hooks/        # Custom React hooks
‚îú‚îÄ‚îÄ docs/             # Documentation
‚îú‚îÄ‚îÄ scripts/          # Build and utility scripts
‚îî‚îÄ‚îÄ public/           # Static assets
```

## Technology Stack

- **Framework:** Next.js 15 with App Router
- **Desktop Runtime:** Tauri (to be configured in Phase 1.2)
- **Language:** TypeScript
- **Styling:** Tailwind CSS
- **AI Services:** OpenAI (Whisper + GPT)

## Documentation

- **[API Key Setup Guide](docs/API_KEY_SETUP.md)** - How to get and configure OpenAI API keys
- **[Complete Setup Guide](docs/SETUP.md)** - Comprehensive setup instructions for all platforms
- **[Quick Reference](docs/QUICK_REFERENCE.md)** - Common commands and workflows
- **[Environment Setup Guide](docs/ENVIRONMENT_SETUP.md)** - Detailed environment configuration
- **[WSL Development Guide](docs/WSL_DEVELOPMENT.md)** - WSL-specific setup and workflow
- **[Dependency Management Guide](docs/DEPENDENCY_MANAGEMENT.md)** - Managing and updating dependencies
- **[Dependency Quick Reference](docs/DEPENDENCY_QUICK_REFERENCE.md)** - Quick dependency commands
- **[Code Signing Guide](docs/CODE_SIGNING.md)** - Code signing setup for distribution
- **[Code Signing Quick Start](docs/CODE_SIGNING_QUICK_START.md)** - Quick reference for code signing
- **[Task List](.kiro/specs/tasklist.md)** - Development roadmap and tasks
- **[Requirements](.kiro/specs/requirements.md)** - Product requirements and user stories
- **[Design Document](.kiro/specs/design.md)** - Architecture and design decisions

## Troubleshooting

### Common Issues

**npm install fails or is very slow in WSL:**
```bash
# Use npm cache
npm config set cache /tmp/npm-cache
npm install
```

**File watching not working:**
```bash
# Increase inotify watchers
echo fs.inotify.max_user_watches=524288 | sudo tee -a /etc/sysctl.conf
sudo sysctl -p
```

**Port 3000 already in use:**
```bash
# Kill process on port 3000
lsof -ti:3000 | xargs kill -9

# Or use a different port
npm run dev -- -p 3001
```

**Environment validation fails:**
- Ensure `.env.local` exists in the project root
- Verify `OPENAI_API_KEY` is set correctly
- Restart the development server after changing environment variables

**Cannot access localhost from Windows (WSL):**
```bash
# Check WSL IP address
ip addr show eth0

# Access using WSL IP instead of localhost
# http://<wsl-ip>:3000
```

For more troubleshooting help, see:
- [WSL Development Guide](docs/WSL_DEVELOPMENT.md#common-issues-and-solutions)
- [Environment Setup Guide](docs/ENVIRONMENT_SETUP.md#troubleshooting)

## Project Status

This project is currently in **Phase 1: Project Setup & Foundation**. See [Task List](.kiro/specs/tasklist.md) for detailed progress.

**Completed:**
- ‚úÖ Next.js project initialization
- ‚úÖ TypeScript configuration
- ‚úÖ ESLint and Prettier setup
- ‚úÖ Tailwind CSS configuration
- ‚úÖ Basic folder structure
- ‚úÖ Environment variable system
- ‚úÖ WSL development workflow
- ‚úÖ Automated setup script

**Next Steps:**
- ‚è≠Ô∏è Configure Tauri desktop runtime (Phase 1.2)
- ‚è≠Ô∏è Implement audio recording (Phase 2)
- ‚è≠Ô∏è Add transcription service (Phase 4)

## Contributing

This is a private project. For development guidelines:
1. Follow the existing code style (enforced by ESLint/Prettier)
2. Run `npm run lint` before committing
3. Test changes in WSL environment
4. Update documentation for new features

## License

Private project - All rights reserved
